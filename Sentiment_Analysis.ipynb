{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy.random import seed\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.learning_curve import learning_curve, validation_curve\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from scipy import interp\n",
    "from itertools import combinations\n",
    "import pyprind\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:42\n"
     ]
    }
   ],
   "source": [
    "# Assemble all text documents into CSV file\n",
    "pbar = pyprind.ProgBar(50000) # number of documents to read\n",
    "labels = {'pos':1, 'neg':0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = '/home/sandeepubuntu/Downloads/aclImdb/%s/%s' %(s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path,file),'r') as infile:\n",
    "                txt = infile.read()\n",
    "                \n",
    "            df = df.append([[txt,labels[l]]],ignore_index=True)\n",
    "            pbar.update()\n",
    "            \n",
    "            \n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the date and store it in CSV\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('./movie_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never, ever, have I been as impressed by a fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This piece of garbage belongs in the basement ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arg. The shuffling dinosaurs are back to take ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Never, ever, have I been as impressed by a fil...          1\n",
       "1  This piece of garbage belongs in the basement ...          0\n",
       "2  Arg. The shuffling dinosaurs are back to take ...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV\n",
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 5)\t2\n",
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of words model example \n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bags = count.fit_transform(docs)\n",
    "print(bags)\n",
    "print(count.vocabulary_)\n",
    "print(bags.toarray())\n",
    "\n",
    "# Every column is the tf of the dictionary items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "# Assessing the weights of the words using TF-IDF\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision = 2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", go and see it - the hours float by very quickly!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data \n",
    "print(df.loc[0,'review'][-50:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function : Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) + \\\n",
    "                        ' '.join(emoticons).replace('-','')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example: \n",
    "preprocessor(df.loc[0,'review'][-50:])\n",
    "\n",
    "# Applying preprocessor to the entire dataset\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']\n",
      "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print(tokenizer('runners like running and thus they run'))\n",
    "print(tokenizer_porter('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'thu', 'run']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words like 'is , are, the, and' \n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('runners like running and thus they run') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 124.4min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 158.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training and test data \n",
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents = None, \n",
    "                        lowercase= False,\n",
    "                        preprocessor = None)\n",
    "\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop,None],\n",
    "               'vect__tokenizer': [tokenizer,tokenizer_porter],\n",
    "               'clf__penalty': ['l1','l2'],\n",
    "               'clf__C': [1.0,10.0,100.0]}, \n",
    "              \n",
    "             { 'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop,None],\n",
    "               'vect__tokenizer': [tokenizer,tokenizer_porter],\n",
    "               'vect__use_idf': [False],\n",
    "               'vect__norm': [None],\n",
    "               'clf__penalty': ['l1','l2'],\n",
    "               'clf__C': [1.0,10.0,100.0]}\n",
    "             ]\n",
    "\n",
    "\n",
    "lr_tfidf = Pipeline([ ('vect',tfidf), \n",
    "                      ('clf', LogisticRegression(random_state=0))\n",
    "                    ])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf,param_grid, scoring='accuracy', cv = 5, verbose = 1,n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7f376fabce18>}\n",
      "\n",
      "CV Best Score: 0.8927\n",
      "\n",
      "Test Score: 0.8986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analyzing the model and predictin result\n",
    "print(gs_lr_tfidf.best_params_)\n",
    "print('\\nCV Best Score: %0.4f'%gs_lr_tfidf.best_score_)\n",
    "clf  = gs_lr_tfidf.best_estimator_\n",
    "print('\\nTest Score: %0.4f'%( clf.score(X_test,y_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
